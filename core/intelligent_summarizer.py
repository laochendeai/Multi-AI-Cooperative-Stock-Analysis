#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡æ¡£ç²¾ç®€ç³»ç»Ÿ - æ›¿æ¢ç®€å•æˆªå–ï¼Œæ”¯æŒå…³é”®ä¿¡æ¯æå–å’Œå†…å®¹æ‘˜è¦ç”Ÿæˆ
"""

import re
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import asyncio

logger = logging.getLogger(__name__)

class IntelligentSummarizer:
    """æ™ºèƒ½æ–‡æ¡£ç²¾ç®€å™¨"""
    
    def __init__(self, max_length: int = 1000):
        self.max_length = max_length
        
        # å…³é”®è¯æƒé‡é…ç½®
        self.keyword_weights = {
            # æŠ•èµ„ç›¸å…³é«˜æƒé‡è¯
            "investment": 3.0, "æŠ•èµ„": 3.0, "å»ºè®®": 3.0, "æ¨è": 3.0,
            "buy": 2.5, "sell": 2.5, "hold": 2.5, "ä¹°å…¥": 2.5, "å–å‡º": 2.5, "æŒæœ‰": 2.5,
            "risk": 2.5, "é£é™©": 2.5, "æœºä¼š": 2.5, "opportunity": 2.5,
            
            # åˆ†æç›¸å…³ä¸­æƒé‡è¯
            "analysis": 2.0, "åˆ†æ": 2.0, "è¯„ä¼°": 2.0, "assessment": 2.0,
            "trend": 2.0, "è¶‹åŠ¿": 2.0, "é¢„æµ‹": 2.0, "forecast": 2.0,
            "performance": 2.0, "è¡¨ç°": 2.0, "ä¸šç»©": 2.0,
            
            # è´¢åŠ¡ç›¸å…³è¯
            "revenue": 1.8, "è¥æ”¶": 1.8, "profit": 1.8, "åˆ©æ¶¦": 1.8,
            "growth": 1.8, "å¢é•¿": 1.8, "valuation": 1.8, "ä¼°å€¼": 1.8,
            "earnings": 1.8, "æ”¶ç›Š": 1.8, "è´¢åŠ¡": 1.8, "financial": 1.8,
            
            # å¸‚åœºç›¸å…³è¯
            "market": 1.5, "å¸‚åœº": 1.5, "price": 1.5, "ä»·æ ¼": 1.5,
            "volume": 1.5, "æˆäº¤é‡": 1.5, "sentiment": 1.5, "æƒ…ç»ª": 1.5,
            
            # æŠ€æœ¯åˆ†æè¯
            "technical": 1.5, "æŠ€æœ¯": 1.5, "indicator": 1.5, "æŒ‡æ ‡": 1.5,
            "support": 1.5, "æ”¯æ’‘": 1.5, "resistance": 1.5, "é˜»åŠ›": 1.5,
            
            # æ–°é—»äº‹ä»¶è¯
            "news": 1.3, "æ–°é—»": 1.3, "event": 1.3, "äº‹ä»¶": 1.3,
            "announcement": 1.3, "å…¬å‘Š": 1.3, "policy": 1.3, "æ”¿ç­–": 1.3
        }
        
        # å¥å­é‡è¦æ€§æ ‡è¯†ç¬¦
        self.importance_markers = {
            "ç»“è®º": 3.0, "conclusion": 3.0, "æ€»ç»“": 3.0, "summary": 3.0,
            "å»ºè®®": 2.8, "recommendation": 2.8, "æ¨è": 2.8,
            "é‡è¦": 2.5, "important": 2.5, "å…³é”®": 2.5, "key": 2.5,
            "æ ¸å¿ƒ": 2.3, "core": 2.3, "ä¸»è¦": 2.3, "main": 2.3,
            "æ˜¾è‘—": 2.0, "significant": 2.0, "æ˜æ˜¾": 2.0, "obvious": 2.0
        }
    
    def summarize_content(self, content: str, context: str = "", 
                         preserve_structure: bool = True) -> str:
        """æ™ºèƒ½ç²¾ç®€å†…å®¹"""
        try:
            if not content or len(content.strip()) <= self.max_length:
                return content.strip()
            
            # é¢„å¤„ç†å†…å®¹
            cleaned_content = self._preprocess_content(content)
            
            # å¦‚æœæ¸…ç†åçš„å†…å®¹ä»ç„¶è¿‡é•¿ï¼Œè¿›è¡Œæ™ºèƒ½ç²¾ç®€
            if len(cleaned_content) > self.max_length:
                if preserve_structure:
                    summarized = self._structure_aware_summarize(cleaned_content, context)
                else:
                    summarized = self._sentence_based_summarize(cleaned_content, context)
                
                # æ·»åŠ ç²¾ç®€æ ‡è¯†
                if len(summarized) < len(cleaned_content):
                    summarized += f"\n\nğŸ’¡ *å†…å®¹å·²æ™ºèƒ½ç²¾ç®€ï¼ŒåŸæ–‡é•¿åº¦: {len(content)}å­—ç¬¦*"
                
                return summarized
            else:
                return cleaned_content
                
        except Exception as e:
            logger.error(f"å†…å®¹ç²¾ç®€å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æˆªå–
            return content[:self.max_length] + "...\n\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­æ˜¾ç¤º]"
    
    def _preprocess_content(self, content: str) -> str:
        """é¢„å¤„ç†å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\s+', ' ', content)
        content = re.sub(r'\n\s*\n', '\n\n', content)
        
        # ç§»é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·
        content = re.sub(r'[ã€‚ï¼ï¼Ÿ]{2,}', 'ã€‚', content)
        content = re.sub(r'[ï¼Œã€ï¼›ï¼š]{2,}', 'ï¼Œ', content)
        
        # ç§»é™¤æ ¼å¼åŒ–å­—ç¬¦
        content = content.replace("**", "").replace("*", "")
        content = content.replace("###", "").replace("##", "").replace("#", "")
        content = content.replace("```", "").replace("`", "")
        
        # ç§»é™¤å¤šä½™çš„å¼•å·å’Œæ‹¬å·
        content = re.sub(r'["""]{2,}', '"', content)
        content = re.sub(r'[ï¼ˆï¼‰()]{2,}', '', content)
        
        return content.strip()
    
    def _structure_aware_summarize(self, content: str, context: str = "") -> str:
        """ç»“æ„æ„ŸçŸ¥çš„ç²¾ç®€"""
        # åˆ†æå†…å®¹ç»“æ„
        sections = self._identify_sections(content)
        
        if sections:
            # æŒ‰é‡è¦æ€§æ’åºæ®µè½
            ranked_sections = self._rank_sections(sections, context)
            
            # é€‰æ‹©æœ€é‡è¦çš„æ®µè½
            selected_content = []
            current_length = 0
            
            for section, score in ranked_sections:
                if current_length + len(section) <= self.max_length * 0.9:  # ç•™10%ç©ºé—´ç»™æ ‡è¯†
                    selected_content.append(section)
                    current_length += len(section)
                else:
                    # å¦‚æœæ®µè½å¤ªé•¿ï¼Œè¿›è¡Œå¥å­çº§ç²¾ç®€
                    remaining_space = self.max_length * 0.9 - current_length
                    if remaining_space > 100:  # è‡³å°‘è¦æœ‰100å­—ç¬¦ç©ºé—´
                        summarized_section = self._sentence_based_summarize(
                            section, context, int(remaining_space)
                        )
                        selected_content.append(summarized_section)
                    break
            
            return "\n\n".join(selected_content)
        else:
            # å¦‚æœæ— æ³•è¯†åˆ«ç»“æ„ï¼Œä½¿ç”¨å¥å­çº§ç²¾ç®€
            return self._sentence_based_summarize(content, context)
    
    def _identify_sections(self, content: str) -> List[str]:
        """è¯†åˆ«å†…å®¹æ®µè½"""
        # æŒ‰åŒæ¢è¡Œåˆ†å‰²æ®µè½
        sections = [section.strip() for section in content.split('\n\n') if section.strip()]
        
        # å¦‚æœæ®µè½å¤ªå°‘ï¼ŒæŒ‰å•æ¢è¡Œåˆ†å‰²
        if len(sections) < 3:
            sections = [section.strip() for section in content.split('\n') if section.strip()]
        
        # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½
        sections = [section for section in sections if len(section) > 20]
        
        return sections
    
    def _rank_sections(self, sections: List[str], context: str = "") -> List[Tuple[str, float]]:
        """å¯¹æ®µè½æŒ‰é‡è¦æ€§æ’åº"""
        ranked = []
        
        for section in sections:
            score = self._calculate_section_importance(section, context)
            ranked.append((section, score))
        
        # æŒ‰åˆ†æ•°é™åºæ’åº
        ranked.sort(key=lambda x: x[1], reverse=True)
        
        return ranked
    
    def _calculate_section_importance(self, section: str, context: str = "") -> float:
        """è®¡ç®—æ®µè½é‡è¦æ€§åˆ†æ•°"""
        score = 0.0
        section_lower = section.lower()
        
        # 1. å…³é”®è¯æƒé‡
        for keyword, weight in self.keyword_weights.items():
            count = section_lower.count(keyword.lower())
            score += count * weight
        
        # 2. é‡è¦æ€§æ ‡è¯†ç¬¦
        for marker, weight in self.importance_markers.items():
            if marker.lower() in section_lower:
                score += weight
        
        # 3. æ•°å­—å’Œç™¾åˆ†æ¯”ï¼ˆé€šå¸¸åŒ…å«é‡è¦ä¿¡æ¯ï¼‰
        number_count = len(re.findall(r'\d+\.?\d*%?', section))
        score += number_count * 0.5
        
        # 4. é•¿åº¦æƒ©ç½šï¼ˆé¿å…é€‰æ‹©è¿‡é•¿çš„æ®µè½ï¼‰
        length_penalty = len(section) / 1000  # æ¯1000å­—ç¬¦å‡0.1åˆ†
        score -= length_penalty * 0.1
        
        # 5. ä½ç½®æƒé‡ï¼ˆå¼€å¤´å’Œç»“å°¾çš„æ®µè½é€šå¸¸æ›´é‡è¦ï¼‰
        # è¿™ä¸ªéœ€è¦åœ¨è°ƒç”¨æ—¶ä¼ å…¥ä½ç½®ä¿¡æ¯ï¼Œæš‚æ—¶è·³è¿‡
        
        # 6. ä¸Šä¸‹æ–‡ç›¸å…³æ€§
        if context:
            context_keywords = self._extract_keywords(context)
            for keyword in context_keywords:
                if keyword.lower() in section_lower:
                    score += 1.0
        
        return score
    
    def _sentence_based_summarize(self, content: str, context: str = "", 
                                max_length: Optional[int] = None) -> str:
        """åŸºäºå¥å­çš„ç²¾ç®€"""
        if max_length is None:
            max_length = self.max_length
        
        # åˆ†å‰²å¥å­
        sentences = self._split_sentences(content)
        
        if not sentences:
            return content[:max_length] + "..."
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„é‡è¦æ€§
        sentence_scores = []
        for sentence in sentences:
            score = self._calculate_sentence_importance(sentence, context)
            sentence_scores.append((sentence, score))
        
        # æŒ‰é‡è¦æ€§æ’åº
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©æœ€é‡è¦çš„å¥å­
        selected_sentences = []
        current_length = 0
        
        for sentence, score in sentence_scores:
            if current_length + len(sentence) <= max_length * 0.9:
                selected_sentences.append(sentence)
                current_length += len(sentence)
            else:
                break
        
        # æŒ‰åŸå§‹é¡ºåºé‡æ–°æ’åˆ—
        original_order = []
        for sentence in sentences:
            if sentence in selected_sentences:
                original_order.append(sentence)
        
        return "ã€‚".join(original_order) + "ã€‚" if original_order else content[:max_length] + "..."
    
    def _split_sentences(self, content: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        # ä½¿ç”¨ä¸­è‹±æ–‡å¥å·åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', content)
        
        # æ¸…ç†å’Œè¿‡æ»¤
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        
        return sentences
    
    def _calculate_sentence_importance(self, sentence: str, context: str = "") -> float:
        """è®¡ç®—å¥å­é‡è¦æ€§"""
        score = 0.0
        sentence_lower = sentence.lower()
        
        # 1. å…³é”®è¯æƒé‡
        for keyword, weight in self.keyword_weights.items():
            if keyword.lower() in sentence_lower:
                score += weight
        
        # 2. é‡è¦æ€§æ ‡è¯†ç¬¦
        for marker, weight in self.importance_markers.items():
            if marker.lower() in sentence_lower:
                score += weight
        
        # 3. æ•°å­—å’Œç™¾åˆ†æ¯”
        number_count = len(re.findall(r'\d+\.?\d*%?', sentence))
        score += number_count * 0.8
        
        # 4. å¥å­é•¿åº¦ï¼ˆé€‚ä¸­é•¿åº¦çš„å¥å­é€šå¸¸åŒ…å«æ›´å¤šä¿¡æ¯ï¼‰
        length = len(sentence)
        if 50 <= length <= 200:
            score += 1.0
        elif length < 20:
            score -= 1.0
        elif length > 300:
            score -= 0.5
        
        # 5. ä¸Šä¸‹æ–‡ç›¸å…³æ€§
        if context:
            context_keywords = self._extract_keywords(context)
            for keyword in context_keywords:
                if keyword.lower() in sentence_lower:
                    score += 1.5
        
        return score
    
    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        words = re.findall(r'\b\w+\b', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'å› ä¸º', 'æ‰€ä»¥', 
                     'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
        
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        # è¿”å›å‡ºç°é¢‘ç‡è¾ƒé«˜çš„è¯
        from collections import Counter
        word_counts = Counter(keywords)
        
        return [word for word, count in word_counts.most_common(10)]
    
    def extract_key_points(self, content: str, max_points: int = 5) -> List[str]:
        """æå–å…³é”®è¦ç‚¹"""
        try:
            sentences = self._split_sentences(content)
            
            if not sentences:
                return []
            
            # è®¡ç®—å¥å­é‡è¦æ€§
            sentence_scores = []
            for sentence in sentences:
                score = self._calculate_sentence_importance(sentence)
                sentence_scores.append((sentence, score))
            
            # é€‰æ‹©æœ€é‡è¦çš„å¥å­ä½œä¸ºè¦ç‚¹
            sentence_scores.sort(key=lambda x: x[1], reverse=True)
            
            key_points = []
            for sentence, score in sentence_scores[:max_points]:
                # ç®€åŒ–å¥å­
                simplified = self._simplify_sentence(sentence)
                if simplified:
                    key_points.append(simplified)
            
            return key_points
            
        except Exception as e:
            logger.error(f"æå–å…³é”®è¦ç‚¹å¤±è´¥: {e}")
            return []
    
    def _simplify_sentence(self, sentence: str) -> str:
        """ç®€åŒ–å¥å­"""
        # ç§»é™¤ä¿®é¥°è¯
        sentence = re.sub(r'(éå¸¸|å¾ˆ|æ¯”è¾ƒ|ç›¸å½“|ååˆ†|æå…¶)', '', sentence)
        
        # ç§»é™¤å†—ä½™è¡¨è¾¾
        sentence = re.sub(r'(æˆ‘è®¤ä¸º|æˆ‘è§‰å¾—|æ®æˆ‘æ‰€çŸ¥|æ ¹æ®åˆ†æ)', '', sentence)
        
        # ä¿ç•™æ ¸å¿ƒä¿¡æ¯
        sentence = sentence.strip()
        
        return sentence if len(sentence) > 10 else ""
    
    def create_executive_summary(self, content: str, max_length: int = 200) -> str:
        """åˆ›å»ºæ‰§è¡Œæ‘˜è¦"""
        try:
            # æå–å…³é”®è¦ç‚¹
            key_points = self.extract_key_points(content, max_points=3)
            
            if not key_points:
                # å¦‚æœæ— æ³•æå–è¦ç‚¹ï¼Œä½¿ç”¨å¥å­çº§ç²¾ç®€
                return self._sentence_based_summarize(content, max_length=max_length)
            
            # ç»„åˆå…³é”®è¦ç‚¹
            summary = "ã€‚".join(key_points) + "ã€‚"
            
            # å¦‚æœæ‘˜è¦ä»ç„¶è¿‡é•¿ï¼Œè¿›ä¸€æ­¥ç²¾ç®€
            if len(summary) > max_length:
                summary = self._sentence_based_summarize(summary, max_length=max_length)
            
            return summary
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ‰§è¡Œæ‘˜è¦å¤±è´¥: {e}")
            return content[:max_length] + "..."


class ContentProcessor:
    """å†…å®¹å¤„ç†å™¨ - é›†æˆæ™ºèƒ½ç²¾ç®€åŠŸèƒ½"""
    
    def __init__(self, max_length: int = 1000):
        self.summarizer = IntelligentSummarizer(max_length)
    
    def process_analysis_content(self, content: Any, agent_type: str = "", 
                               context: str = "") -> str:
        """å¤„ç†åˆ†æå†…å®¹"""
        try:
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if isinstance(content, dict):
                content_str = self._extract_from_dict(content)
            elif isinstance(content, list):
                content_str = "\n".join([str(item) for item in content])
            else:
                content_str = str(content)
            
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
            if not content_str or len(content_str.strip()) < 20:
                return "æš‚æ— åˆ†ææ•°æ®"
            
            # æ™ºèƒ½ç²¾ç®€
            processed = self.summarizer.summarize_content(
                content_str, 
                context=f"{agent_type} {context}",
                preserve_structure=True
            )
            
            return processed if processed else "åˆ†æç»“æœå¤„ç†å¤±è´¥"
            
        except Exception as e:
            logger.error(f"å¤„ç†åˆ†æå†…å®¹å¤±è´¥: {e}")
            return f"å†…å®¹å¤„ç†å¤±è´¥: {str(e)}"
    
    def _extract_from_dict(self, data: Dict[str, Any]) -> str:
        """ä»å­—å…¸ä¸­æå–å†…å®¹"""
        # ä¼˜å…ˆå­—æ®µ
        priority_fields = [
            "content", "analysis", "summary", "conclusion", 
            "recommendation", "result", "output", "response"
        ]
        
        # å°è¯•ä»ä¼˜å…ˆå­—æ®µæå–
        for field in priority_fields:
            if field in data and data[field]:
                return str(data[field])
        
        # å¦‚æœæ²¡æœ‰ä¼˜å…ˆå­—æ®µï¼Œç»„åˆæ‰€æœ‰æœ‰æ„ä¹‰çš„å­—æ®µ
        excluded_fields = {
            "agent_id", "timestamp", "status", "confidence", 
            "model_used", "type", "metadata"
        }
        
        content_parts = []
        for key, value in data.items():
            if key not in excluded_fields and value:
                if isinstance(value, (str, int, float)):
                    content_parts.append(f"{key}: {value}")
                elif isinstance(value, dict):
                    nested_content = self._extract_from_dict(value)
                    if nested_content:
                        content_parts.append(f"{key}: {nested_content}")
        
        return "\n".join(content_parts)
    
    def create_summary_report(self, analysis_results: Dict[str, Any]) -> Dict[str, str]:
        """åˆ›å»ºæ‘˜è¦æŠ¥å‘Š"""
        try:
            summary_report = {}
            
            # å¤„ç†å„ä¸ªåˆ†æéƒ¨åˆ†
            analysis_sections = [
                ("market_analysis", "å¸‚åœºåˆ†æ"),
                ("sentiment_analysis", "æƒ…æ„Ÿåˆ†æ"),
                ("news_analysis", "æ–°é—»åˆ†æ"),
                ("fundamentals_analysis", "åŸºæœ¬é¢åˆ†æ"),
                ("bull_arguments", "å¤šå¤´è§‚ç‚¹"),
                ("bear_arguments", "ç©ºå¤´è§‚ç‚¹"),
                ("investment_recommendation", "æŠ•èµ„å»ºè®®"),
                ("trading_strategy", "äº¤æ˜“ç­–ç•¥"),
                ("risk_assessment", "é£é™©è¯„ä¼°"),
                ("final_decision", "æœ€ç»ˆå†³ç­–")
            ]
            
            for key, name in analysis_sections:
                if key in analysis_results:
                    content = analysis_results[key]
                    processed = self.process_analysis_content(
                        content, 
                        agent_type=name,
                        context="æŠ•èµ„åˆ†æ"
                    )
                    summary_report[key] = processed
            
            return summary_report
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ‘˜è¦æŠ¥å‘Šå¤±è´¥: {e}")
            return {}
    
    def extract_key_insights(self, analysis_results: Dict[str, Any]) -> List[str]:
        """æå–å…³é”®æ´å¯Ÿ"""
        try:
            all_content = []
            
            # æ”¶é›†æ‰€æœ‰åˆ†æå†…å®¹
            for key, value in analysis_results.items():
                if isinstance(value, str) and len(value) > 50:
                    all_content.append(value)
            
            combined_content = "\n\n".join(all_content)
            
            # æå–å…³é”®è¦ç‚¹
            key_points = self.summarizer.extract_key_points(
                combined_content, 
                max_points=8
            )
            
            return key_points
            
        except Exception as e:
            logger.error(f"æå–å…³é”®æ´å¯Ÿå¤±è´¥: {e}")
            return []
